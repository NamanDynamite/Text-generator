{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV8l3SJHzdHe",
        "outputId": "339fb6f4-c2c7-400d-df3b-d9351d3976e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1238.8412, Val Loss: 151.3972\n",
            "Epoch 2, Train Loss: 1093.7753, Val Loss: 146.9364\n",
            "Epoch 3, Train Loss: 1013.8013, Val Loss: 143.5484\n",
            "Epoch 4, Train Loss: 942.9048, Val Loss: 142.7269\n",
            "Epoch 5, Train Loss: 876.9266, Val Loss: 140.6242\n",
            "Epoch 6, Train Loss: 813.3304, Val Loss: 139.2903\n",
            "Epoch 7, Train Loss: 752.3231, Val Loss: 138.9309\n",
            "Epoch 8, Train Loss: 695.2009, Val Loss: 139.0736\n",
            "Epoch 9, Train Loss: 642.9313, Val Loss: 140.1995\n",
            "Epoch 10, Train Loss: 590.7052, Val Loss: 143.5858\n",
            "Epoch 11, Train Loss: 544.3588, Val Loss: 146.5291\n",
            "Epoch 12, Train Loss: 503.1327, Val Loss: 150.2346\n",
            "Epoch 13, Train Loss: 467.9070, Val Loss: 152.4617\n",
            "Epoch 14, Train Loss: 435.9606, Val Loss: 155.0246\n",
            "Epoch 15, Train Loss: 405.4795, Val Loss: 157.3082\n",
            "Epoch 16, Train Loss: 379.4858, Val Loss: 160.6913\n",
            "Epoch 17, Train Loss: 352.4193, Val Loss: 165.7646\n",
            "Epoch 18, Train Loss: 330.1144, Val Loss: 170.4040\n",
            "Epoch 19, Train Loss: 309.1825, Val Loss: 172.2117\n",
            "Epoch 20, Train Loss: 289.3488, Val Loss: 175.0789\n",
            "Epoch 21, Train Loss: 270.4358, Val Loss: 177.2943\n",
            "Epoch 22, Train Loss: 251.0693, Val Loss: 179.7102\n",
            "Epoch 23, Train Loss: 229.5303, Val Loss: 183.0562\n",
            "Epoch 24, Train Loss: 209.7021, Val Loss: 184.7962\n",
            "Epoch 25, Train Loss: 195.9529, Val Loss: 185.3049\n",
            "Epoch 26, Train Loss: 179.0097, Val Loss: 187.9947\n",
            "Epoch 27, Train Loss: 165.3026, Val Loss: 191.3310\n",
            "Epoch 28, Train Loss: 153.7405, Val Loss: 194.5191\n",
            "Epoch 29, Train Loss: 145.0885, Val Loss: 197.2861\n",
            "Epoch 30, Train Loss: 134.2423, Val Loss: 202.5120\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading LanguageTool 6.5: 100%|██████████| 248M/248M [00:04<00:00, 61.4MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmpaoqc7ys4.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-6.5.zip to /root/.cache/language_tool_python.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The global surface temperatures of the moon in a lag between heating and cooling of diviner channels in proc. 7th lunar pyroclastic deposits. Icarus, with a cloudy atmosphere to map pixels, fine-grained materials and most prominent impacts are consistent with this is apparent around craters. Icarus,\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import language_tool_python  \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "with open(\"Chandrayaan-3 Mission.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s.,!?']+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d+\\b\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "text = clean_text(raw_text)\n",
        "words = text.split()\n",
        "word_freq = Counter(words)\n",
        "vocab = sorted(word_freq.keys())\n",
        "\n",
        "\n",
        "vocab.append(\"<END>\")\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "encoded = [word2idx[w] for w in words]\n",
        "\n",
        "def create_dataset(data, seq_length=20):\n",
        "    inputs, targets = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        inputs.append(data[i:i+seq_length])\n",
        "        targets.append(data[i+1:i+seq_length+1])\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "seq_length = 20\n",
        "inputs, targets = create_dataset(encoded, seq_length)\n",
        "\n",
        "split_idx = int(0.9 * len(inputs))\n",
        "train_x, val_x = inputs[:split_idx], inputs[split_idx:]\n",
        "train_y, val_y = targets[:split_idx], targets[split_idx:]\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, n_heads=8, num_layers=2, max_len=100):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, dropout=0.1)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        assert seq_len <= self.pos_embedding.size(1), \"Sequence length exceeds max_len\"\n",
        "        x = self.embedding(x) + self.pos_embedding[:, :seq_len, :]\n",
        "        x = self.dropout(self.norm(x))\n",
        "        x = x.permute(1, 0, 2)  # [seq_len, batch_size, embed_dim]\n",
        "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        return self.fc(x)\n",
        "\n",
        "model = TinyTransformer(vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i in range(0, len(train_x), batch_size):\n",
        "        x_batch = train_x[i:i+batch_size].to(device)\n",
        "        y_batch = train_y[i:i+batch_size].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x_batch)\n",
        "        loss = loss_fn(output.view(-1, vocab_size), y_batch.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for i in range(0, len(val_x), batch_size):\n",
        "            x_batch = val_x[i:i+batch_size].to(device)\n",
        "            y_batch = val_y[i:i+batch_size].to(device)\n",
        "            output = model(x_batch)\n",
        "            loss = loss_fn(output.view(-1, vocab_size), y_batch.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"tiny_transformer_word.pth\")\n",
        "\n",
        "\n",
        "def top_k_sampling(logits, k=50, temperature=1.0):\n",
        "    logits = logits / temperature\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    probs = F.softmax(sorted_logits[:, :k], dim=-1)\n",
        "    return sorted_indices[0, torch.multinomial(probs, 1).item()].item()\n",
        "\n",
        "def top_p_sampling(logits, p=0.95):\n",
        "    logits = logits / 1.0\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "    sorted_indices_to_keep = cumulative_probs <= p\n",
        "    sorted_indices_to_keep[..., 0] = 1\n",
        "    filtered_logits = sorted_logits.masked_fill(~sorted_indices_to_keep, -float('Inf'))\n",
        "    probs = F.softmax(filtered_logits, dim=-1)\n",
        "    return sorted_indices[0, torch.multinomial(probs, 1).item()].item()\n",
        "\n",
        "def apply_repetition_penalty(logits, generated_tokens, penalty=1.5):\n",
        "    token_counts = Counter(generated_tokens)\n",
        "    for token_id, count in token_counts.items():\n",
        "        if count > 1:\n",
        "            logits[0, token_id] /= (penalty * count)\n",
        "    return logits\n",
        "\n",
        "def generate_text(model, prompt, length=60, temperature=0.8, top_p=0.97, top_k=50, repetition_penalty=1.5):\n",
        "    model.eval()\n",
        "    prompt_tokens = [word2idx.get(w, 0) for w in prompt.lower().split()]\n",
        "    input_ids = torch.tensor([prompt_tokens]).to(device)\n",
        "    generated = prompt_tokens.copy()\n",
        "\n",
        "    for _ in range(length):\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids)[:, -1, :]\n",
        "            logits = apply_repetition_penalty(output.clone(), generated, penalty=repetition_penalty)\n",
        "            next_token_id = top_k_sampling(logits, k=top_k, temperature=temperature)\n",
        "\n",
        "            if idx2word[next_token_id] == \"<END>\" or idx2word[next_token_id] in [\".\", \"!\", \"?\"]:\n",
        "                break\n",
        "\n",
        "            generated.append(next_token_id)\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]]).to(device)], dim=1)\n",
        "\n",
        "    return ' '.join([idx2word[idx] for idx in generated])\n",
        "\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "def correct_grammar(text):\n",
        "    matches = tool.check(text)\n",
        "    return language_tool_python.utils.correct(text, matches)\n",
        "\n",
        "\n",
        "prompt = \"The global surface temperatures of the Moon\"\n",
        "generated_text = generate_text(model, prompt, length=60, temperature=0.6, top_k=35, top_p=0.88, repetition_penalty=3)\n",
        "corrected_text = correct_grammar(generated_text)\n",
        "\n",
        "print(corrected_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPOtYAbUBZ2_",
        "outputId": "c13e4840-2f19-4007-cca1-194b6af8ef64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.9.2-py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (2025.1.31)\n",
            "Downloading language_tool_python-2.9.2-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install language_tool_python\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
